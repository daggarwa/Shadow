#include <ros/ros.h>
#include <image_transport/image_transport.h>
#include <cv_bridge/cv_bridge.h>
#include <sensor_msgs/image_encodings.h>

#include <opencv2/video/tracking.hpp>
#include <opencv2/video/background_segm.hpp>
#include <opencv2/imgproc/imgproc.hpp>
//#include <opencv2/videoio/videoio.hpp>
#include <opencv2/highgui/highgui.hpp>
#include "iostream"
#include "string.h"

static const std::string OPENCV_WINDOW = "Image window";

class ImageConverter
{
private:
    ros::NodeHandle nh_;
    image_transport::ImageTransport it_;
    image_transport::Subscriber image_sub_;
    image_transport::Publisher image_pub_;
    
    cv::Mat flow, cflow, frame;
    cv::Mat current_grayframe, prev_grayframe, uflow;
    // namedWindow("flow", 1);
 

public:
    ImageConverter()
      : it_(nh_)
    {
        // Subscrive to input video feed and publish output video feed
        image_sub_ = it_.subscribe("/usb_cam/image_raw", 1, 
          &ImageConverter::imageCb, this);
        image_pub_ = it_.advertise("/image_converter/output_video", 1);

        cv::namedWindow(OPENCV_WINDOW);
    }

    ~ImageConverter()
    {
        cv::destroyWindow(OPENCV_WINDOW);
    }

    void imageCb(const sensor_msgs::ImageConstPtr& msg)
    {
        cv_bridge::CvImagePtr cv_ptr;
        try
        {
            cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);
            // cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::MONO8);
            // current_grayframe = cv_ptr->image;
            cvtColor(cv_ptr->image, current_grayframe, CV_BGR2GRAY);
        }
        catch (cv_bridge::Exception& e)
        {
            ROS_ERROR("cv_bridge exception: %s", e.what());
            return;
        }
    }

    static void drawOptFlowMap(const cv::Mat& flow, cv::Mat& cflowmap, int step,
                        double)
    {
        for(int y = 0; y < cflowmap.rows; y += step)
            for(int x = 0; x < cflowmap.cols; x += step)
            {
                const cv::Point2f& fxy = flow.at<cv::Point2f>(y, x);
                cv::line(cflowmap, cv::Point(x,y), cv::Point(cvRound(x+fxy.x), cvRound(y+fxy.y)),
                     CV_RGB(0, 255, 0));
                cv::circle(cflowmap, cv::Point(x,y), 2, CV_RGB(0, 255, 0), -1);
            }
    }

    std::string type2str(int type) {
    	std::string r;
    
    	uchar depth = type & CV_MAT_DEPTH_MASK;
    	uchar chans = 1 + (type >> CV_CN_SHIFT);
    
    	switch ( depth ) {
    		case CV_8U:  r = "8U"; break;
    		case CV_8S:  r = "8S"; break;
    		case CV_16U: r = "16U"; break;
    		case CV_16S: r = "16S"; break;
    		case CV_32S: r = "32S"; break;
    		case CV_32F: r = "32F"; break;
    		case CV_64F: r = "64F"; break;
    		default:     r = "User"; break;
    	}
    
    	r += "C";
    	r += (chans+'0');
    
    	return r;
    }

    void MOG2BackgroundSubtraction()
    {
        // cv::Ptr<cv::BackgroundSubtractor> pMOG2; //MOG2 Background subtractor
        //create Background Subtractor objects
        // pMOG2 = cv::createBackgroundSubtractorMOG2(); //MOG2 approach
        cv::Mat fgMaskMOG2; //fg mask fg mask generated by MOG2 method
        sensor_msgs::ImagePtr out_frame;
        cv::Mat rgb_flow, flow_th, flow_th_c;

	cv::BackgroundSubtractorMOG2 bg(5, 64);

	int flow_dilation_type = cv::MORPH_ELLIPSE, dilation_size = 9;
	int flow_erosion_type = cv::MORPH_ELLIPSE, erosion_size = 2;
	cv::Mat element_flow_dil = getStructuringElement(flow_dilation_type, cv::Size(9,9));
	cv::Mat element_flow_ero = getStructuringElement(flow_erosion_type, cv::Size(2, 2));

	cv::vector<cv::vector<cv::Point> > contours;
	cv::vector<cv::Vec4i> hierarchy;

        ros::Rate loop_rate(10);

        while(nh_.ok())
        {
            if( !current_grayframe.empty() )
            {
		bg.operator()(current_grayframe, fgMaskMOG2);

		cv::dilate(fgMaskMOG2, fgMaskMOG2, element_flow_ero, cv::Point(-1,-1), 1); // confirm same src/dst works
		cv::erode(fgMaskMOG2, fgMaskMOG2, element_flow_ero, cv::Point(-1,-1), 1); // confirm same src/dst works
		//cv::dilate(fgMaskMOG2, fgMaskMOG2, element_flow_dil, cv::Point(-1,-1), 3); // confirm same src/dst works

		cv::GaussianBlur(fgMaskMOG2, fgMaskMOG2, cv::Size(3,3), 0, 0);
		//cv::dilate(fgMaskMOG2, fgMaskMOG2, element_flow_dil, cv::Point(-1,-1), 3); // confirm same src/dst works
		cv::threshold(fgMaskMOG2, flow_th, 50, 255, CV_THRESH_BINARY);

		// Contour Finding
		flow_th_c = flow_th.clone();
		cv::findContours(flow_th_c, contours, hierarchy, CV_RETR_CCOMP, CV_CHAIN_APPROX_SIMPLE, cv::Point(0, 0));

		// Approximate contours to polygons + get bounding rects
  		cv::vector<cv::vector<cv::Point> > contours_poly( contours.size() );
  		cv::vector<cv::Rect> boundRect;
  		cv::vector<cv::Rect> boundRectRefined;
		int count = 0;
		cv::Scalar color = cv::Scalar(255, 0, 0);
		for( int i = 0; i< contours.size(); i++ )
		{
		    cv::approxPolyDP(cv::Mat(contours[i]), contours_poly[i], 3, true);
		    if ((contourArea(contours_poly[i]) < 500) || (contours_poly[i].size() < 20))
		    {
		        //ROS_INFO("Skipped for area: %f size: ", contourArea(contours[i]));
		        continue;
		    }
		    boundRect.push_back(cv::boundingRect(cv::Mat(contours_poly[i])));
		    count++;
		}    
		

		// We need to make sure that no two bounding rectangles are overlapping
		// In case they are, we should merge both of them into a single rectangle that contains both the ones.

		int flag[boundRect.size()];
		cv::Rect intersect, rectElement;
		for( int i = 0; i< boundRect.size(); i++ ) {
		    if (flag[i] == 1)
			continue;
		    rectElement = boundRect[i];
		    for( int j = i+1; j< boundRect.size(); j++ ) {
			intersect = (boundRect[i] & boundRect[j]);
			if (intersect.area() > 0) {
			    flag[j] = 1;
			    rectElement = (rectElement | boundRect[j]);
			}
		    }
		    boundRectRefined.push_back(rectElement);
		}

		for( int i = 0; i< boundRectRefined.size(); i++ )
		    cv::rectangle(current_grayframe, boundRectRefined[i].tl(), boundRectRefined[i].br(), color, 2, 8, 0);		    


        	//update the background model
        	// pMOG2->apply(current_grayframe, fgMaskMOG2);
                cv::imshow(OPENCV_WINDOW, fgMaskMOG2);
                cv::imshow("THRESHOLD", flow_th);
                cv::imshow("DETECT", current_grayframe);
                cv::waitKey(1);

                // cv_bridge::CvImage out_msg;
                // out_msg.header   = in_msg->header; // Same timestamp and tf frame as input image
                // out_msg.encoding = sensor_msgs::image_encodings::TYPE_32FC1; // Or whatever
                // out_msg.image    = sal_float_image; // Your cv::Mat
                
                out_frame = cv_bridge::CvImage(std_msgs::Header(), "bgr8", rgb_flow).toImageMsg();

                // Output modified video stream
                image_pub_.publish(out_frame);
            }
            ros::spinOnce();
            loop_rate.sleep();
        }
    }

    void OpticalFlow()
    {
 
        double  min, max;

        // cv_bridge::CvImage out_frame;
        sensor_msgs::ImagePtr out_frame;

        cv::Mat flowxy_mag, flowx, flowy, image, prev_rgbframe, flow_th, flow_th2, flow_th_c;

        cv::Mat rgb_flow, prev_frame_edge;
    
	int flow_dilation_type_mag = cv::MORPH_ELLIPSE, dilation_size_mag = 10;
	int flow_dilation_type = cv::MORPH_ELLIPSE, dilation_size = 15;
	int flow_erosion_type = cv::MORPH_ELLIPSE, erosion_size = 5;
	cv::Mat element = getStructuringElement(flow_dilation_type_mag, cv::Size(10, 10));
	cv::Mat element_flow_dil = getStructuringElement(flow_dilation_type, cv::Size(15,15));
	cv::Mat element_flow_ero = getStructuringElement(flow_erosion_type, cv::Size(5, 5));

	cv::vector<cv::vector<cv::Point> > contours;
	cv::vector<cv::Vec4i> hierarchy;

        ros::Rate loop_rate(10);

        while(nh_.ok())
        {
            if( !current_grayframe.empty() )
            {
		// Operation on Flow Image 
                cv::calcOpticalFlowFarneback(prev_grayframe, current_grayframe, flow, 0.5, 1, 5, 3, 5, 1.1, cv::OPTFLOW_FARNEBACK_GAUSSIAN);
                cv::vector<cv::Mat> planes(2);
                split(flow, planes);
                flowx = planes[0];
                flowy = planes[1];        
                magnitude(flowx, flowy, flowxy_mag);
		flowxy_mag.convertTo(flowxy_mag, CV_8U);

		int PATCH_SIZE = 15;
		int THRESH = 5;

		for(int i=PATCH_SIZE; i<flowxy_mag.rows-PATCH_SIZE; i++)
    		    for(int j=PATCH_SIZE; j<flowxy_mag.cols-PATCH_SIZE; j++) {
			if (flowxy_mag.at<uchar>(i, j) > THRESH) {
			    cv::Mat roi = flowxy_mag(cv::Rect(j, i, PATCH_SIZE, PATCH_SIZE));
			    cv::minMaxLoc(roi, NULL, &max);
			    if (max < 20)
				flowxy_mag.at<uchar>(i, j) = 0;
			}
		    }

		//cv::GaussianBlur(flowxy_mag, flowxy_mag, cv::Size(3,3), 0, 0);
		cv::threshold(flowxy_mag, flow_th, THRESH, 255, CV_THRESH_BINARY);

		// Morphological Operation on Thresholded Flow Image
		cv::dilate(flow_th, flow_th, element, cv::Point(-1,-1), 3); // confirm same src/dst works
		cv::erode(flow_th, flow_th, element_flow_ero, cv::Point(-1,-1), 1); // confirm same src/dst works

		// Contour Finding
		flow_th_c = flow_th.clone();
		cv::findContours(flow_th_c, contours, hierarchy, CV_RETR_CCOMP, CV_CHAIN_APPROX_SIMPLE, cv::Point(0, 0));

		// Approximate contours to polygons + get bounding rects
  		cv::vector<cv::vector<cv::Point> > contours_poly( contours.size() );
  		cv::vector<cv::Rect> boundRect;
  		cv::vector<cv::Rect> boundRectRefined;
		int count = 0;
		cv::Scalar color = cv::Scalar(255, 0, 0);
		for( int i = 0; i< contours.size(); i++ )
		{
		    cv::approxPolyDP(cv::Mat(contours[i]), contours_poly[i], 3, true);
		    if ((contourArea(contours_poly[i]) < 500) || (contours_poly[i].size() < 20))
		    {
		        //ROS_INFO("Skipped for area: %f size: ", contourArea(contours[i]));
		        continue;
		    }
		    boundRect.push_back(cv::boundingRect(cv::Mat(contours_poly[i])));
		    count++;
		}    
		

		// We need to make sure that no two bounding rectangles are overlapping
		// In case they are, we should merge both of them into a single rectangle that contains both the ones.

		int flag[boundRect.size()];
		cv::Rect intersect, rectElement;
		for( int i = 0; i< boundRect.size(); i++ ) {
		    if (flag[i] == 1)
			continue;
		    rectElement = boundRect[i];
		    for( int j = i+1; j< boundRect.size(); j++ ) {
			intersect = (boundRect[i] & boundRect[j]);
			if (intersect.area() > 0) {
			    flag[j] = 1;
			    rectElement = (rectElement | boundRect[j]);
			}
		    }
		    boundRectRefined.push_back(rectElement);
		}

		for( int i = 0; i< boundRectRefined.size(); i++ )
		    cv::rectangle(current_grayframe, boundRectRefined[i].tl(), boundRectRefined[i].br(), color, 2, 8, 0);		    

		//// Visualization of Flow Image
                // colorizeFlow(flowx, flowy, image);
                // drawOptFlowMap(flow, prev_rgbframe, 16, 1.5);


                //cv::imshow("PREV EDGE FRAME", prev_frame_edge);
                cv::imshow(OPENCV_WINDOW, flow_th);
                cv::imshow("FLOW MAG", flowxy_mag);
                cv::imshow("DETECT", current_grayframe);
                cv::waitKey(1);

                // cv_bridge::CvImage out_msg;
                // out_msg.header   = in_msg->header; // Same timestamp and tf frame as input image
                // out_msg.encoding = sensor_msgs::image_encodings::TYPE_32FC1; // Or whatever
                // out_msg.image    = sal_float_image; // Your cv::Mat
                
                out_frame = cv_bridge::CvImage(std_msgs::Header(), "bgr8", rgb_flow).toImageMsg();

                // Output modified video stream
                image_pub_.publish(out_frame);
            }
            ros::spinOnce();
            loop_rate.sleep();
            std::swap(prev_grayframe, current_grayframe);
        }
    }
};

int main(int argc, char** argv)
{
    ros::init(argc, argv, "image_converter");
    ImageConverter ic;
    //ic.OpticalFlow();
    ic.MOG2BackgroundSubtraction();
    ros::spin();
    return 0;
}
